{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to manipulate some of the classification algorithms of [scikit-learn](http://scikit-learn.org/stable/documentation.html). \n",
    "\n",
    "This notebook was created by [Chloé-Agathe Azencott](http://cazencott.info)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created using\n",
    "* python 3.4.3\n",
    "* numpy 1.15.0\n",
    "* matplotlib 2.2.2\n",
    "* scikit-learn 0.19.2\n",
    "\n",
    "You can check your version of Python by running\n",
    "```python\n",
    "import sys\n",
    "print(sys.version)\n",
    "```\n",
    "\n",
    "and the version of any module by running\n",
    "```python\n",
    "import <module name>\n",
    "print(<module name>.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data science libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicting mushroom edibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a data set that describes mushrooms according to the shape of their cap and stalk, their odor, the type of their veil, etc. This data set also contains information on whether a mushroom is edible or not, and that is what we will try to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are available as `data/mushrooms.csv`. Let us load them in a pandas DataFrame called `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mushrooms.csv', \n",
    "                 #index_col=0 # the first column is the IDs of the samples\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the first few lines of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Converting the features into numerical values\n",
    "\n",
    "As you can see, the features are encoded as _letters_. Each letter correspond to a category . For example, for the `cap shape` feature, `b` corresponds to a bell cap, `c` to a conical cap, `f` to a flat cap, `k` to a knobbed cap, `s` to a sunken cap, and `x` to a convex cap. For more details about their meaning, you can consult [the documentation of the data set](https://archive.ics.uci.edu/ml/datasets/Mushroom).\n",
    "\n",
    "In order to work with this data, we need to convert the categorical attributes into numerical values. Here we will simply convert each letter to a number between 0 and the number of categories, using scikit-learn's [preprocessing.LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\n",
    "\n",
    "This encoding is not necessarily the best, as (for example), an algorithm that uses the Euclidean distance will consider that a convex cap (`x` converted to 5) is closer to a sunken cap (`s` converted to 4) than to a conical cap (`c` converted to 1), and the [one-hot encoding](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features) is a good alternative. However, it has the drawback of increasing the number of features, and of creating correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "for col in df.columns:\n",
    "    df[col] = labelencoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2 Split the data into a train set and a test set\n",
    "\n",
    "Let us first extact the design matrix `X` and the label vector `y` from the data frame.\n",
    "\n",
    "The first column, `class`, is our target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, 1:]) # exclude first column\n",
    "y = np.array(df.iloc[:, 0])  # first column only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 70% of the samples of the data for training our models, and the remaining samples to evaluate them.\n",
    "\n",
    "Let us create the numpy arrays `Xtrain`, `Xtest` and the vectors `ytrain`, `ytest` that we will work with, using scikit-learn's [model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "Xtrain, Xtest, ytrain, ytest = model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question :__ How many training and test samples do we have? How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.3 Model selection by cross-validation\n",
    "Remember, using the test data to select our best model is likely to result in _overfitting_. \n",
    "\n",
    "We will therefore start by splitting the training data into 5 folds of cross-validation. Scikit-learn also does that for us.\n",
    "\n",
    "scikit-learn model selection: http://scikit-learn.org/stable/model_selection.html#model-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Créate a KFold object\n",
    "kf = model_selection.KFold(n_splits=5,  # 5 folds\n",
    "                           shuffle=True # shuffle the samples before splitting \n",
    "                          )\n",
    "\n",
    "# Use kf to split Xtrain into 5 folds. \n",
    "# kf.split returns an iterator ((that would be consumed after one iteration). \n",
    "# We transform it in a list (on which we can iterate as often as we want).\n",
    "kf_indices = list(kf.split(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, (tr, te) in enumerate(kf_indices):\n",
    "    print(\"Training data for fold %d:\" % i, tr)\n",
    "    print(\"Test data for fold %d:\" % i, te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Linear models\n",
    "\n",
    "We will use scikit-learn to train a few [linear models](http://scikit-learn.org/stable/modules/linear_model.html#linear-model) on `(Xtrain, ytrain)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.1 Logistic regression\n",
    "How does a logistic regression perform on this data? We'll evaluate it by cross-validation, using the cross-validation folds we have generated above.\n",
    "\n",
    "But wait! What will our criterion be? There are several you can choose from on http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter. In this lab, we will use the [F1 score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question :__ What other criterion or criteria could you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Annswer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a logistic regression model. We will use [scikit-learn's implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), which includes a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ What is the relationship between the regularization parameter `C` and the regularization parameter `alpha` of the [ridge regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(C=1e10) # use a large penalty \n",
    "                                                # so as not to regularize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn allows us to directly compute the cross-validated F1 score (on the train test) of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model_selection.cross_val_score(model, \n",
    "                                     Xtrain, y=ytrain, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "print([\"%.3f\" % value for value in f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ What is the average f1 score of the linear regression over all folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What are the weights that a logistic regression assigns to each variable on the train set? To determine this, we will train a logistic regression on the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a logistic regression on the entire train set\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize(12, 6))\n",
    "\n",
    "# Plot, for each feature, its coefficient in the model\n",
    "num_features = Xtrain.shape[1]\n",
    "plt.scatter(range(num_features), model.coef_)\n",
    "\n",
    "plt.xlabel('Feature', fontsize=14)\n",
    "feature_names = list(df.columns[1:])\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "tmp = plt.ylabel('Weight', fontsize=14)\n",
    "\n",
    "tmp = plt.title('Logistic regression weights', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Which feature do you think is the most important to predict edibility? Which one is the least important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.2 Ridge logistic regression\n",
    "\n",
    "Can we improve performance using regularization? By default, the logistic regression implemented in scikit-learn uses a l2 (i.e. ridge) penalty. Let us now try to find the optimal value of the regularization parameter `C`.\n",
    "\n",
    "Let us create 50 values of `C` for testing, equally spaced (in log scale) between $10^{-1}$ and $10^4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cvalues = np.logspace(-2, 3, 50)\n",
    "\n",
    "f1_per_c = [] # will store the f1 values for all 50 values of C\n",
    "weights_per_c = [] #  will store the weights associated with each feature \n",
    "                       # for all 50 values of C\n",
    "for cval in cvalues:\n",
    "    # Create a logistic regression ridge model\n",
    "    model = linear_model.LogisticRegression(C=cval)\n",
    "    \n",
    "    # Compute the model's cross-validated performance\n",
    "    f1 = model_selection.cross_val_score(model, \n",
    "                                         Xtrain, y=ytrain, \n",
    "                                         scoring='f1', \n",
    "                                         cv=kf_indices)\n",
    "    f1_per_c.append(f1)\n",
    "    \n",
    "    # Train the model on the entire data set and store the regression coefficients\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    weights_per_c.append(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the weights assigned to each feature with C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape weights_per_c into a (number of values of C, number of features) array\n",
    "weights_per_c = np.array(weights_per_c)\n",
    "weights_per_c.shape = (50, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot, for each feature, the regression weight as a function of alpha\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "lines = plt.plot(cvalues, weights_per_c)\n",
    "plt.xscale('log')\n",
    "tmp = plt.legend(lines, feature_names, frameon=False, \n",
    "                 loc=(1, 0), fontsize=14)\n",
    "\n",
    "tmp = plt.xlabel('C', fontsize=14)\n",
    "tmp = plt.ylabel('Weight', fontsize=14)\n",
    "\n",
    "tmp = plt.title('Logistic ridge regression weights', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Is this consistent with what we observed for the non-regularized logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of the F1 score with C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Plot the f1 score (averaged across the 5 folds) as a function of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ What is the optimal value of C and what is the corresponding average cross-validated f1? Use `np.argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search of the optimal parameter\n",
    "\n",
    "Scikit-learn's [model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) allows us to automatically select the hyperparameters of a learning algorithm that are optimal, in cross-validation, on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of parameter values:\n",
    "param_grid = {'C': cvalues}\n",
    "\n",
    "# Define a model that will do a grid search:\n",
    "model = model_selection.GridSearchCV(linear_model.LogisticRegression(),\n",
    "                                     param_grid, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# Best parameter\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Are those results coherent with prior observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with optimal parameters has already been trained on the entire training set and is accessible via `model.best_estimator_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ How does the regularized logistic ridge regression compare to the vanilla logistic regression? Are you surprised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Non-linear models\n",
    "Can a non-linear model improve the performance on this data? Our performance are already very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1 Decision trees\n",
    "\n",
    "Let us start with a simple non-linear models: a decision tree. They are implemented in scikit-learn's [tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a DT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the cross-validated F1 score (on the train test) of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model_selection.cross_val_score(model, \n",
    "                                     Xtrain, y=ytrain, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "print([\"%.3f\" % value for value in f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ What is the average f1 score of the decision tree over all folds? How does it compare to the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Decision Tree\n",
    "\n",
    "Wow! Our decision tree seems to be *a perfect model*! We can look at its structure in more details, using the code provided in [scikit-learn's documentation](http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model on the entire train set:\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "n_nodes = model.tree_.node_count\n",
    "children_left = model.tree_.children_left\n",
    "children_right = model.tree_.children_right\n",
    "feature = model.tree_.feature\n",
    "threshold = model.tree_.threshold\n",
    "\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the **importance** of each feature, which is the total reduction in Gini criterion that is brought by this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize(12, 6))\n",
    "\n",
    "# Plot, for each feature, its importance in the model\n",
    "plt.scatter(range(num_features), model.feature_importances_)\n",
    "\n",
    "plt.xlabel('Feature', fontsize=14)\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "tmp = plt.ylabel('Weight', fontsize=14)\n",
    "\n",
    "tmp = plt.title('DT importance', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Which features are the more important? Which ones are not used by the model? How does this compare to the linear models we trained before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(model.feature_importances_)\n",
    "print([feature_names[ix] for ix in sorted_indices[-5:]])\n",
    "print(model.feature_importances_[sorted_indices[-5:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the problem harder\n",
    "Because this data set is an \"easy\" data set -- we achieved very good performance with a linear model, and perfection with a decision tree --, it is difficult to use it to illustrate the differences between several non-linear models.\n",
    "\n",
    "In what follows, we are making the problem harder by introducing noise in the labeled data: We will randomly flip some of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of ytrain\n",
    "ytrain_hard = np.array(ytrain)\n",
    "\n",
    "# randomly flip some of the labels:\n",
    "# define which samples to flip\n",
    "where_to_flip = np.random.binomial(1, 0.05, size=ytrain.shape)\n",
    "print(\"Randomly flipping %d labels\" % np.sum(where_to_flip))\n",
    "\n",
    "# flip ytrain where where_to_flip is not zero\n",
    "ytrain_hard = np.where(where_to_flip == 0, ytrain, 1-ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees on the more difficult data set\n",
    "\n",
    "Let us now work with this harder training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the cross-validated F1 score (on the train test) of a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model_selection.cross_val_score(model, \n",
    "                                     Xtrain, y=ytrain_hard, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "print([\"%.3f\" % value for value in f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ What is the average f1 score of this decision tree over the 5 folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "print(\"%.3f\" % np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat the experience once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tree.DecisionTreeClassifier()\n",
    "f1_2 = model_selection.cross_val_score(model2, \n",
    "                                     Xtrain, y=ytrain_hard, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "print([\"%.3f\" % value for value in f1_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Do you observe the same f1 scores on each fold as previously? Is this surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model on the more difficult data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Is a linear model better or worse than a decision tree on this more difficult data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Compare, on the same plot, the feature importance given by each of the decision trees and the feature weights in a linear model. (You may need to re-scale the feature weights.) Do the three models give the same importance to the same features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Can an ensemble method improve the performance of the decision tree on the difficult data set? We will use the random forest implementation in scikit-learn's [ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important hyperparameter of a random forest is the number of trees it contain. We will therefore vary this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees_values = [10, 20, 50, 100, 300, 500, 2000]\n",
    "\n",
    "f1_per_ntrees = [] # will store the f1 values for all values of ntrees\n",
    "importance_per_ntrees = [] #  will store the importance of each feature \n",
    "                          # for all values of ntrees\n",
    "    \n",
    "for ntrees in ntrees_values:\n",
    "    # Create a random forest model\n",
    "    model = ensemble.RandomForestClassifier(n_estimators=ntrees)\n",
    "    \n",
    "    # Compute the model's cross-validated performance\n",
    "    f1 = model_selection.cross_val_score(model, \n",
    "                                         Xtrain, y=ytrain_hard, \n",
    "                                         scoring='f1', \n",
    "                                         cv=kf_indices)\n",
    "    f1_per_ntrees.append(f1)\n",
    "    \n",
    "    # Train the model on the entire data set and store the regression coefficients\n",
    "    model.fit(Xtrain, ytrain_hard)\n",
    "    importance_per_ntrees.append(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "__Question:__ Which number of trees yields the optimal cross-validated f1 score? What is this score? How does it compare to the scores of the previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Let us now look at the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape importance_per_ntree into a (number of values of ntree, number of features) array\n",
    "importance_per_ntrees = np.array(importance_per_ntrees)\n",
    "importance_per_ntrees.shape = (len(ntrees_values), X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot, for each feature, the regression weight as a function of alpha\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "lines = plt.plot(ntrees_values, importance_per_ntrees)\n",
    "plt.xscale('log')\n",
    "tmp = plt.legend(lines, feature_names, frameon=False, \n",
    "                 loc=(1, 0), fontsize=14)\n",
    "\n",
    "tmp = plt.xlabel('Number of trees', fontsize=14)\n",
    "tmp = plt.ylabel('Importance', fontsize=14)\n",
    "\n",
    "tmp = plt.title('Random forest importance', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Which features do you now think are the most important to predict edibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classifiers are implemented in scikit-learn's [svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "Let us check the cross-validated performance of an SVM with RBF kernel on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(-1, 2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linspace(0.01, 0.5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of parameter values:\n",
    "param_grid = {'C': np.logspace(-1, 2, 5), \n",
    "              'gamma': np.linspace(0.01, 0.5, 5)}\n",
    "\n",
    "# Define a model that will do a grid search:\n",
    "model = model_selection.GridSearchCV(svm.SVC(kernel='rbf'),\n",
    "                                     param_grid, \n",
    "                                     scoring='f1', \n",
    "                                     cv=kf_indices)\n",
    "\n",
    "# Train the model\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", model.best_params_)\n",
    "\n",
    "# Best F1 score\n",
    "print(\"Best F1 score: %.3f\" % model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How does the SVM compare to previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Final model\n",
    "\n",
    "__Question:__ Based on the above analyses, which model do you finally choose as the most performant (on the difficult data)? What is its confusion matrix __on the test set?__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
